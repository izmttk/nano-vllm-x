# Micro-vLLM

ËØ•È°πÁõÆÂèó [nano-vllm](https://github.com/GeeeekExplorer/nano-vllm/tree/main) ÂêØÂèëÔºåÊèê‰æõ‰∏Ä‰∏™‰ªéÈõ∂ÂºÄÂßãÊûÑÂª∫ÁöÑ LLM Êé®ÁêÜÊ°ÜÊû∂

Á®ãÂ∫èÁöÑÊû∂ÊûÑÈÅµÂæ™‰∫Ü [vLLM](https://github.com/vllm-project/vllm) v1 Áõ∏‰ººÁöÑÁªÑÁªáÂÆâÊéíÔºå‰ΩÜÊòØ‰∏é vLLM ‰∏çÂêåÁöÑÊòØÔºåËØ•È°πÁõÆÁöÑ KV ÁºìÂ≠òÁ≥ªÁªü‰ΩøÁî®ÁöÑÊòØ [SGLang](https://github.com/sgl-project/sglang) ÁöÑ Radix Cache ÂÆûÁé∞„ÄÇÊâÄ‰ª•ÂèØ‰ª•ÁêÜËß£‰∏∫ÔºåËøô‰∏™È°πÁõÆÊòØ vLLM Âíå SGLang ÊùÇ‰∫§ÁöÑ‰∫ßÁâ© ü§£„ÄÇ

## Features

- ËΩªÈáè‰ΩÜÂÆåÊï¥ÁöÑ‰ª£Á†ÅÂÆûÁé∞
- ÊåÅÁª≠ÊâπÂ§ÑÁêÜÔºàContinuous BatchingÔºâ
- OpenAI ÂÖºÂÆπÁöÑ API
- Âü∫‰∫é Radix Tree ÁöÑ Prefix Caching
- Âº†ÈáèÂπ∂Ë°åÔºàTensor ParallelismÔºâ
- CUDA Graph ÊîØÊåÅÔºà‰ªÖ Decoding Èò∂ÊÆµÔºâ

## Requirements

```plaintext
torch >= 2.6.0
transformers >= 4.50.0
fastapi >= 0.95.0
flashinfer-python >= 0.2.0
psutil
```

## Quick Start

ÂêØÂä® API ÊúçÂä°

```bash
python3 -m entrypoints.openai.api --model <model_path> --host 0.0.0.0 --port 8000
```

Offline Inference

```py
from core.llm import LLM

async main(prompt: str, *args, **kwargs):
    llm = LLM(*args, **kwargs)
    async for token in llm.generate(
        prompt,
        SamplingParams(
            max_new_tokens=50,
            temperature=0.6,
            top_p=0.95,
            top_k=20,
        )
    ):
        print(token, end="", flush=True)
```

## Benchmarks

Experiment Environment:

- GPU: A100 40GB
- Model: Qwen3-0.6B
- Number of Requests: 256
- Prompt Length: random 100 ~ 1024
- Generation Length: random 100 ~ 1024
- Script: [bench.py](bench.py)

Results:

| Inference Engine | Output Tokens | Time (s) | Throughput (tokens/s) |
|------------------|---------------|----------|-----------------------|
| vLLM v0.11.0     | 133966        | 18.24    |  7343.96              |
| ours             | 133966        | 14.83    |  9032.37              |

## TODO

- Graceful Shutdown
- Better Logging System
- Benchmark Metrics on API Server
- Pipeline Parallelism
- More Configurable Options

[WIP]